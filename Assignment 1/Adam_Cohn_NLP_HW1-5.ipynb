{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit ('yandex': virtualenv)",
      "language": "python",
      "name": "python37564bityandexvirtualenv005add28e08e42ddb94988cf771fa745"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5-final"
    },
    "colab": {
      "name": "Adam Cohn - NLP_HW1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ieia83_Vu7UR",
        "colab_type": "text"
      },
      "source": [
        "## Sentence reformulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxbzWf1Yu7US",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCylCNjDu7UZ",
        "colab_type": "text"
      },
      "source": [
        "Download FastText pretrained vectors for English: \n",
        "[cc.en300.vec.gz](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz)\n",
        "\n",
        "And download Yelp! dataset composed of reviews: \n",
        "[Yelp.train.text](https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCAxJyBou7Ua",
        "colab_type": "code",
        "outputId": "0c0765fd-c45b-4428-9e08-18ca11c0e024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!wget -nc https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz \n",
        "!gunzip -k cc.en.300.vec.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘cc.en.300.vec.gz’ already there; not retrieving.\n",
            "\n",
            "gzip: cc.en.300.vec already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KOdIUHfu7Ue",
        "colab_type": "text"
      },
      "source": [
        "Load downloaded pretrained FastText vectors by gensim library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3Ayx4OSu7Uf",
        "colab_type": "code",
        "outputId": "5e3fd07f-d106-471f-f0c9-4dd2f2e503d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "fast = KeyedVectors.load_word2vec_format('cc.en.300.vec')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ3Qso4pu7Ui",
        "colab_type": "text"
      },
      "source": [
        "Compute similarity of two words using gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoZUiWU8u7Ui",
        "colab_type": "code",
        "outputId": "7fbe9e8c-5aec-4b0b-cf68-5f1ea82571d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "#We discussed different words, look and similarity of 'king' and 'queen' for example. Could you put it inot context?\n",
        "\n",
        "#your code here\n",
        "print(f'The similiarity between the vectors for the words \"king\" and \"queen\" is {fast.similarity(\"king\", \"queen\")}.')\n",
        "print(f'The distance between \"king\" and \"queen\" is {fast.distance(\"king\", \"queen\")}')\n",
        "print(fast.distance(\"king\", \"queen\") + fast.similarity(\"king\", \"queen\"))\n",
        "print(f'The similiarity between the vectors for the words \"king\" and \"man\" is {fast.similarity(\"king\", \"man\")}.')\n",
        "print(f'The distance between \"king\" and \"man\" is {fast.distance(\"king\", \"man\")}')\n",
        "print(fast.distance(\"king\", \"man\") + fast.similarity(\"king\", \"man\"))\n",
        "print(f'The similiarity between the vectors for the words \"king\" and \"woman\" is {fast.similarity(\"king\", \"woman\")}.')\n",
        "print(f'The distance between \"king\" and \"woman\" is {fast.distance(\"king\", \"woman\")}')\n",
        "print(fast.distance(\"king\", \"woman\") + fast.similarity(\"king\", \"woman\"))\n",
        "print(f'The similiarity between the vectors for the words \"queen\" and \"woman\" is {fast.similarity(\"queen\", \"woman\")}.')\n",
        "print(f'The distance between \"queen\" and \"woman\" is {fast.distance(\"queen\", \"woman\")}')\n",
        "print(fast.distance(\"queen\", \"woman\") + fast.similarity(\"queen\", \"woman\"))\n",
        "print(f'The similiarity between the vectors for the words \"queen\" and \"man\" is {fast.similarity(\"queen\", \"man\")}.')\n",
        "print(f'The distance between \"queen\" and \"man\" is {fast.distance(\"queen\", \"man\")}')\n",
        "print(fast.distance(\"queen\", \"man\") + fast.similarity(\"queen\", \"man\"))\n",
        "print(f'The similiarity between the vectors for the words \"woman\" and \"man\" is {fast.similarity(\"woman\", \"man\")}.')\n",
        "print(f'The distance between \"woman\" and \"man\" is {fast.distance(\"woman\", \"man\")}')\n",
        "print(fast.distance(\"woman\", \"man\") + fast.similarity(\"woman\", \"man\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The similiarity between the vectors for the words \"king\" and \"queen\" is 0.7069182991981506.\n",
            "The distance between \"king\" and \"queen\" is 0.29308170080184937\n",
            "1.0\n",
            "The similiarity between the vectors for the words \"king\" and \"man\" is 0.341844379901886.\n",
            "The distance between \"king\" and \"man\" is 0.658155620098114\n",
            "1.0\n",
            "The similiarity between the vectors for the words \"king\" and \"woman\" is 0.22299695014953613.\n",
            "The distance between \"king\" and \"woman\" is 0.7770030498504639\n",
            "1.0\n",
            "The similiarity between the vectors for the words \"queen\" and \"woman\" is 0.3607560992240906.\n",
            "The distance between \"queen\" and \"woman\" is 0.6392439007759094\n",
            "1.0\n",
            "The similiarity between the vectors for the words \"queen\" and \"man\" is 0.22957992553710938.\n",
            "The distance between \"queen\" and \"man\" is 0.7704200744628906\n",
            "1.0\n",
            "The similiarity between the vectors for the words \"woman\" and \"man\" is 0.7658364176750183.\n",
            "The distance between \"woman\" and \"man\" is 0.2341635823249817\n",
            "1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY2fkzFIu7Um",
        "colab_type": "text"
      },
      "source": [
        "We can see that the similarity between the word vectors for \"queen\" and \"king\" is very high, approximately 70%.\n",
        "The distance between the words is approximately 0.29, and in fact the distance between any two words = (1 - similarity)\n",
        "\n",
        "We can also see that the distance between \"queen\"-\"woman\" is almost the same as the distance between \"king\"-\"man\", and the same holds for the distance between \"queen\"-\"man\" and \"king\"-\"woman\"., while the distance between \"man\"-\"woman\" is almost the same as \"queen\"-\"king\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2hp2_Y_u7Um",
        "colab_type": "text"
      },
      "source": [
        "Sentence tokenization. Split Yelp! texts into separate tokens (words and punctuation marks) by space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6NAmjpqwSMl",
        "colab_type": "code",
        "outputId": "9cde7d7e-8b55-4365-f8c9-a01440fad4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!wget -nc https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-17 14:14:26--  https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.31.100, 74.125.31.139, 74.125.31.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.31.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/ [following]\n",
            "--2020-05-17 14:14:26--  https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/\n",
            "Reusing existing connection to drive.google.com:443.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/edit [following]\n",
            "--2020-05-17 14:14:26--  https://drive.google.com/file/d/1TAcfL091lKb2LipaUELFteZqJjQu-gMa/edit\n",
            "Reusing existing connection to drive.google.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘1TAcfL091lKb2LipaUELFteZqJjQu-gMa’\n",
            "\n",
            "1TAcfL091lKb2LipaUE     [ <=>                ]  69.03K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-05-17 14:14:27 (31.6 MB/s) - ‘1TAcfL091lKb2LipaUELFteZqJjQu-gMa’ saved [70684]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfcqrK22u7Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#your code here\n",
        "with open('/content/Yelp.train.text') as text:\n",
        "    yelp_data = text.read()\n",
        "    yelp_space_split = yelp_data.split(' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OEAsjGYu7Uo",
        "colab_type": "text"
      },
      "source": [
        "Try part of speech tagging using [NLTK POS-tagger](https://www.nltk.org/book/ch05.html).\n",
        "The function returns list of tuples (word, POS_tag)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42ItmB0R7Lzf",
        "colab_type": "code",
        "outputId": "23be44f2-4f84-46ac-b3d1-93d053f5071a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(yelp_space_split))\n",
        "while '' in yelp_space_split:\n",
        "  yelp_space_split.remove('')\n",
        "print(len(yelp_space_split))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3525352\n",
            "3525352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5spra5Nu7Up",
        "colab_type": "code",
        "outputId": "d00ed38f-075c-42cb-ccd3-2a9b0c239df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "yelp_tokens = nltk.word_tokenize(yelp_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKUxZn8n3yiI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X7za6zLu7Uq",
        "colab_type": "code",
        "outputId": "4fd14cb1-b5a4-4163-c4a5-1e81f6119776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_yelp_space = nltk.pos_tag(yelp_space_split)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlEqNqShu7Us",
        "colab_type": "text"
      },
      "source": [
        "Can you find the most similar word to the given? Can you write a method that returns a list of tuples (word, similarity) in order of decreasing similarity?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh40nO1Fu7Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_most_similar(word, n=10):\n",
        "    results = fast.similar_by_word(word, topn=n)\n",
        "    return results  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1jS3LKcxslj",
        "colab_type": "code",
        "outputId": "884689b7-548f-4667-927f-0861fc09c2ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "find_most_similar('king')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('kings', 0.7550534009933472),\n",
              " ('queen', 0.7069183588027954),\n",
              " ('king-', 0.7060282230377197),\n",
              " ('king.', 0.6811478734016418),\n",
              " ('king.The', 0.6607562303543091),\n",
              " ('King', 0.6591336727142334),\n",
              " ('prince', 0.6495301127433777),\n",
              " ('-king', 0.6278064250946045),\n",
              " ('monarch', 0.618451714515686),\n",
              " ('queen-mother', 0.6070466041564941)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEZhVuJnu7Uu",
        "colab_type": "text"
      },
      "source": [
        "Let's do the simplest reformulation task. We just want to reformulate some sentences replacing an ajective with a similar one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a27SrDYDu7Uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reformulate_sentence(sentence, word_similar=0.7):\n",
        "    # Sentence tokenization\n",
        "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Part of speech tagging\n",
        "    POS_tagged_words = nltk.pos_tag(tokenized_sentence)\n",
        "\n",
        "    reformulated_sentence_words = []\n",
        "    for word, pos_tag in POS_tagged_words:\n",
        "        # If the word is adjective...\n",
        "        if pos_tag in ['JJR', 'JJS', 'JJ']:\n",
        "            try:\n",
        "                # ...look for the word most similar to the given and replace it\n",
        "                # your code here\n",
        "                sim_words = find_most_similar(word, n=10)\n",
        "                for sim_word,sim in sim_words:\n",
        "                    if sim < word_similar:\n",
        "                        continue\n",
        "                    if sim_word.lower() == word:\n",
        "                        continue\n",
        "                    else:\n",
        "                        word = sim_word\n",
        "                        break\n",
        "                reformulated_sentence_words.append(word)\n",
        "            except:\n",
        "                print(f'There is no {word} word in FastText dictionary! ...')\n",
        "        else:\n",
        "            reformulated_sentence_words.append(word)\n",
        "    # Join words list in a sentence\n",
        "    return ' '.join(reformulated_sentence_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsJqDSHau7Uv",
        "colab_type": "code",
        "outputId": "d5ed8d1a-a4e3-4dbb-a039-8d598a8d68a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "reformulate_sentence('I am tall, short, yellow, hungry, beautiful')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am tallish , short , red , famished , gorgeous'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkCCzrXGu7Ux",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9nU6DXUu7Ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYlAGD9qu7Uz",
        "colab_type": "code",
        "outputId": "482b8cb3-ec52-4b07-ae98-05ea6b4d90b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDWCgFSGu7U0",
        "colab_type": "text"
      },
      "source": [
        "VADER sentiment classifier from NLTK library. The range of sentiment is from -1 to 1 where -1 is negative, 0 is neutral and 1 is positive\n",
        "\n",
        "Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe7ubn9Su7U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fasRfBCu7U1",
        "colab_type": "text"
      },
      "source": [
        "Read the dataset text file line by line and put lines into the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIJF9RPxu7U1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTFNfoIju7U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz6Dq-xWu7U4",
        "colab_type": "text"
      },
      "source": [
        "Read Yelp dataset from text file and \n",
        "get 1000 random sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVKhsdAAOEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random \n",
        "\n",
        "yelp_sent_tokenize = nltk.sent_tokenize(yelp_data)\n",
        "res = random.sample(range(0,len(yelp_sent_tokenize)), 1000) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGZNTqzhu7U5",
        "colab_type": "text"
      },
      "source": [
        "Compute average sentiment of 1000 sentences sentences set by VADER sentiment classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-oNwgw_u7U5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum_compound = 0\n",
        "for i in res :\n",
        "    scores = sentiment_analyzer.polarity_scores(yelp_sent_tokenize[i])\n",
        "    sum_compound += scores['compound']\n",
        "\n",
        "avg_compound = sum_compound/(len(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM3sr_GvCjrV",
        "colab_type": "code",
        "outputId": "424c617a-9e28-428a-8007-e00804291e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "avg_compound"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2553403000000005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AownYPoxu7U6",
        "colab_type": "text"
      },
      "source": [
        "Reformulate sentences and compute average sentiment again. Try to come up with ways to make senteces more positive on average. What about more negative? Can you come up with some interesting experiment on this data with POS-tagged reformulations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPsjMUjlEAR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = 'I am tall, short, yellow, hungry, beautiful'\n",
        "sentence =yelp_sent_tokenize[2020]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS9cSkJtpim9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1weNjXuAEO5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reformulate_sentence_mode(sentence, mode ='pos'):\n",
        "    # Sentence tokenization\n",
        "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Part of speech tagging\n",
        "    POS_tagged_words = nltk.pos_tag(tokenized_sentence)\n",
        "\n",
        "    reformulated_sentence_words = []\n",
        "    for word, pos_tag in POS_tagged_words:\n",
        "        # If the word is adjective...\n",
        "        if pos_tag in ['JJR', 'JJS', 'JJ']:\n",
        "            try:\n",
        "                # ...look for the word most similar to the given and replace it\n",
        "                # your code here\n",
        "                sim_words = find_most_similar(word, n=10)\n",
        "                max_word = 0\n",
        "                for sim_word,sim in sim_words:\n",
        "                    analyz = sentiment_analyzer.polarity_scores(sim_word)\n",
        "                    scores = sim * analyz['compound']\n",
        "                    if sim_word.lower() == word:\n",
        "                        continue\n",
        "                    if (mode == 'pos' and scores  > max_word) or (mode == 'neg' and scores  < max_word):\n",
        "                        max_word = scores \n",
        "                        word = sim_word\n",
        "                            \n",
        "                reformulated_sentence_words.append(word)\n",
        "            except:\n",
        "                print(f'There is no {word} word in FastText dictionary! ...')\n",
        "        else:\n",
        "            reformulated_sentence_words.append(word)\n",
        "    # Join words list in a sentence\n",
        "    return ' '.join(reformulated_sentence_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfmgXOZzu7U7",
        "colab_type": "code",
        "outputId": "9fd555b4-80d6-4d58-cefb-8c5526ecb52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print('ROW : ', sentence)\n",
        "analyz = sentiment_analyzer.polarity_scores(sentence)\n",
        "print(analyz)\n",
        "\n",
        "new_sentence = reformulate_sentence_mode(sentence)\n",
        "print('Positive : ' ,new_sentence)\n",
        "analyz = sentiment_analyzer.polarity_scores(new_sentence)\n",
        "print(analyz)\n",
        "\n",
        "\n",
        "new_sentence = reformulate_sentence_mode(sentence,mode='neg')\n",
        "print('Negative: ',new_sentence)\n",
        "analyz = sentiment_analyzer.polarity_scores(new_sentence)\n",
        "print(analyz)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROW :  i was highly disappointed .\n",
            "{'neg': 0.629, 'neu': 0.371, 'pos': 0.0, 'compound': -0.5256}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Positive :  i was highly surprised .\n",
            "{'neg': 0.0, 'neu': 0.477, 'pos': 0.523, 'compound': 0.2944}\n",
            "Negative:  i was highly disheartened .\n",
            "{'neg': 0.636, 'neu': 0.364, 'pos': 0.0, 'compound': -0.5413}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeB1UY69id5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "38319377-4bdd-49f2-eccc-5508e7e71087"
      },
      "source": [
        "sum_compound = 0\n",
        "avg = 0\n",
        "pos = 0\n",
        "neg = 0\n",
        "for i in res :\n",
        "    avg += sentiment_analyzer.polarity_scores(yelp_sent_tokenize[i])['compound']\n",
        "    pos += sentiment_analyzer.polarity_scores(reformulate_sentence_mode(sentence))['compound']\n",
        "    neg += sentiment_analyzer.polarity_scores(reformulate_sentence_mode(sentence,mode='neg'))['compound']\n",
        "avg/=(len(res))\n",
        "pos/=(len(res))\n",
        "neg/=(len(res))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLgIah_0kmyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a96fc6e1-d9f9-4f0c-9370-b32dcb6d22b5"
      },
      "source": [
        "print('Pos : ',pos)\n",
        "print('Avg : ',avg)\n",
        "print('Neg : ',neg)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pos :  0.2943999999999972\n",
            "Avg :  0.2718860000000008\n",
            "Neg :  -0.541299999999991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ6ktsG5lR0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reformulate_sentence_all_option(sentence, word_similar=0.7):\n",
        "    # Sentence tokenization\n",
        "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # Part of speech tagging\n",
        "    POS_tagged_words = nltk.pos_tag(tokenized_sentence)\n",
        "\n",
        "    reformulated_sentence_words = []\n",
        "    for word, pos_tag in POS_tagged_words:\n",
        "        # If the word is adjective...\n",
        "        words = []\n",
        "        words.append(word)\n",
        "        if pos_tag in ['JJR', 'JJS', 'JJ']:\n",
        "            try:\n",
        "                # ...look for the word most similar to the given and replace it\n",
        "                # your code here\n",
        "                sim_words = find_most_similar(word, n=10)\n",
        "                for sim_word,sim in sim_words:\n",
        "                    if sim >word_similar:\n",
        "                        words.append(sim_word)\n",
        "            except:\n",
        "                print(f'There is no {word} word in FastText dictionary! ...')\n",
        "        reformulated_sentence_words.append(words)\n",
        "\n",
        "    return reformulated_sentence_words\n",
        "\n",
        "def rate(x , i=0):\n",
        "    if len(x) == i:\n",
        "        return []\n",
        "    ret_s = []\n",
        "    ends = rate(x , i+1)\n",
        "    if len(ends) == 0 :\n",
        "        return x[i]\n",
        "    for word in x[i]:\n",
        "        for end in ends:\n",
        "            s = word +' ' + end\n",
        "            ret_s.append(s)\n",
        "    return ret_s\n",
        "\n",
        "def pos_neg_sentiment(sentence):\n",
        "    x = reformulate_sentence_all_option(sentence)\n",
        "    score = sentiment_analyzer.polarity_scores(sentence)['compound']\n",
        "    ret_dict = {'pos': (score,sentence) , 'neg':(score,sentence),'normal':(score,sentence)  }\n",
        "    for s in rate(x):\n",
        "        analyz = sentiment_analyzer.polarity_scores(s)['compound']\n",
        "        if (analyz > 0) and (ret_dict['pos'][0]<analyz):\n",
        "            ret_dict['pos'] = (analyz,s)\n",
        "        if (analyz < 0) and (ret_dict['neg'][0]>analyz):\n",
        "            ret_dict['neg'] = (analyz,s)\n",
        "    return ret_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmjTw6bdR1qO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7f43ec92-4dee-43c9-f761-7917c7bc3e95"
      },
      "source": [
        "pos_neg_sentiment(yelp_sent_tokenize[2020])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': (-0.5413, 'i was highly disheartened .'),\n",
              " 'normal': (-0.5256, 'i was highly disappointed .'),\n",
              " 'pos': (0.2944, 'i was highly surprised .')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCSO3LihT2KY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "1ec5b53b-7a07-446b-cb89-c2c26c961ef3"
      },
      "source": [
        "sum_compound = 0\n",
        "avg = 0\n",
        "pos = 0\n",
        "neg = 0\n",
        "for i in res :\n",
        "    scores = pos_neg_sentiment(yelp_sent_tokenize[i])\n",
        "    avg += scores['normal'][0]\n",
        "    pos += scores['pos'][0]\n",
        "    neg += scores['neg'][0]\n",
        "avg/=(len(res))\n",
        "pos/=(len(res))\n",
        "neg/=(len(res))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n",
            "There is no _num_ word in FastText dictionary! ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvOKcsu-V2Zh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4bc7351c-4fd1-4e4a-e4ca-7d3ba2cc22c3"
      },
      "source": [
        "print('Pos : ',pos)\n",
        "print('Avg : ',avg)\n",
        "print('Neg : ',neg)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pos :  0.3292215000000008\n",
            "Avg :  0.2718860000000008\n",
            "Neg :  0.17932970000000026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MdxnaIwWhp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}