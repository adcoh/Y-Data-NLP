{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text generation by Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chain is a probabalistic model in which the probability of each event depends only on the state attained in the previous event. [markovify](https://github.com/jsvine/markovify) is a library for text generation by Markov chain.\n",
    "\n",
    "Use \"pip install markovify\" to install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [Dataset.csv](https://drive.google.com/file/d/1raxIkJZ4lMTvgTB8eYjxu4Ux8aNL-x0s/view?usp=sharing) composed of sarcastic and serious headlines for the news. The csv-file consists of two columns. \"headline\" column contains texts of headlines. \"is_sarcastic\" column contain 0 if the hiadline is serious and 1 otherwise.\n",
    "\n",
    "Read dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "df.drop(columns=\"article_link\",inplace=True)\n",
    "df.head(3)\n",
    "df.to_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            headline  is_sarcastic\n0  former versace store clerk sues over secret 'b...             0\n1  the 'roseanne' revival catches up to our thorn...             0\n2  mom starting to fear son's web series closest ...             1\n3  boehner just wants wife to listen, not come up...             1\n4  j.k. rowling wishes snape happy birthday in th...             0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>is_sarcastic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>former versace store clerk sues over secret 'b...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the 'roseanne' revival catches up to our thorn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mom starting to fear son's web series closest ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>boehner just wants wife to listen, not come up...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>j.k. rowling wishes snape happy birthday in th...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 11724 sarcastic texts in Dataset.csv\nFound 14985 serious texts in Dataset.csv\n"
    }
   ],
   "source": [
    "texts_serious = [] # list of serious hiadline texts\n",
    "texts_sarcastic = [] # list of sarcastic hiadline texts\n",
    "with open('Dataset.csv', encoding='utf-8') as f:\n",
    "    # Rread csv-file by DictReader from csv library\n",
    "    reader = csv.DictReader(f) \n",
    "    for line in reader:\n",
    "        # read texts of headline\n",
    "        headline = line['headline'].strip()\n",
    "        # read sarcasticity of headline\n",
    "        is_sarcastic = int(line['is_sarcastic'].strip())\n",
    "        if is_sarcastic:\n",
    "            texts_sarcastic.append(headline)\n",
    "        else:\n",
    "            texts_serious.append(headline)\n",
    "print('Found {} sarcastic texts in Dataset.csv'.format(len(texts_sarcastic)))\n",
    "print('Found {} serious texts in Dataset.csv'.format(len(texts_serious)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge headlines into one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_0 = '\\n'.join(texts_serious)\n",
    "text_1 = '\\n'.join(texts_sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Markov chain model for serious headlines generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "serious_model = markovify.NewlineText(text_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 serious headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "butch lesbians open up about her son's tv death\napparently andrew wk is a curvy style icon\nrecovery nonprofits stem the tide on inequality and climate change\npamela wright's son was shot dead after southwest airlines flight diverted after ebola virus and sen. barbara boxer can teach your kids will never connect with others\nbusy philipps consoles michelle williams on 10th anniversary of dr. gunn's death, thank an abortion\nswiping right on a crowd of looters in ferguson let high-profile journalists go while charging regular folks with crimes\nlet us down this year than in 2015\nwhy are bi men less likely to hurt its business\nsyrian rebels to exit aleppo as syrian rebels claim to break your heart is hurting immigrant victims of louisiana floods\nkeep the real world\nwhat i wish i'd learned about business from making art\nwhy i want my wife to know on december 21\nmatthew mcconaughey once faked an australian accent for an end to down syndrome is the internet still thinks efforts to gut post-crisis banking regulations\na week â€” and lived to tell if hillary clinton hits trump white house, and it's actually pretty good\n10 of the most beautiful way\nisis vs isil -- what's in hot cars\nfight white supremacy greater than allegiance to white supremacist movement?\nobama honors those who made nazi salute in germany is asking for katie ledecky's autograph\nchanning tatum and jenna dewan tatum slay on the job jason chaffetz won't\nmule-ing it over: high heels and the old guy at lollapalooza?\n"
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(serious_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model and generate 20 sarcastic headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "child slavery gives area activist something to make sure pressing exposed penis against intern doesn't constitute sexual harassment ignored to save relationship\narea woman to celebrate rosh hashasha or something on lawn\n4 billion years to try some more shit\nal franken tearfully announces intention to step down as leader of droogs amidst allegations of biggest sexual harassment apologies\ngene wilder's career in self-storage\neveryone in family sure who else is doing it\nlocal man hates being put on phone told her parents hate her\ncourt rules meryl streep unable to remove mrs. butterworth from table until wife arrives\nreport suggests stalin was just elaborate plan to ramble on about chick he banged last night\niran ready to go until 2016 election\nwildlife cleaning volunteer stuck with the girl from headlines\nmonsanto develops hardier strain of soy flu traced to trouble-making genie\nrush limbaugh tucks shirt back on dog\nthe thinkable happens to local hospital for royal blood transfusion\nobama clears 2,000 square miles of high tensile power lines\nwoman has no time becoming white-collar professional\ncracking sound alerts man he is god\nreport: still a mystery how people in same category as jay z\nclassmates.com employees don't have to deal with all the time burns 5,000 calories an hour\ntrump sits down for the holidays\n"
    }
   ],
   "source": [
    "sarcastic_model = markovify.NewlineText(text_1)\n",
    "for i in range(20):\n",
    "    print(sarcastic_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation by Variation Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of tutorial based on [Text generation with a Variational Autoencoder](https://nicgian.github.io/text-generation-vae/) article. For sentence generation we use Variational Autoencoder (VAE) neural network model that is an extension seq2seq model. Originally VAE was described in [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) paper. The idea behind Variational Autoencoder is that we impose predefined disribution (e.g., normal distribution) on the latent state formed by encoder. On the one hand this restriction alow us to sample random vectors from normal distribution and generate arbitrary sentences. On the othe hand this restriction form very dense well differentiated space without holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "from keras.layers import Bidirectional, Dense, Embedding, \\\n",
    "Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dowload [GloVe](http://nlp.stanford.edu/data/glove.6B.zip) pretrained word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip glove.6b.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 15 # Max text length in tokens\n",
    "MAX_NB_WORDS = 20000 # Max words in dictionary\n",
    "EMBEDDING_DIM = 50 # Dimensionality of GloVe vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sentence tokenizer and two dictionaries: word_to_id and id_to_word\n",
    "\n",
    "For tokenisation we use [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) from keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Corpus has 29656 unique tokens\n"
    }
   ],
   "source": [
    "texts = texts_serious + texts_sarcastic\n",
    "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_to_id = tokenizer.word_index \n",
    "id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "print(f'Corpus has {len(word_to_id)} unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sarcastic texts and create tensor composed of tokens indexes. If a sentence shorter than MAX_SEQUENCE_LENGTH we pad it. If a sentence longer than MAX_SEQUENCE_LENGTH we cut it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1440"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "12*120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data tensor shape: (11724, 15)\n"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts_sarcastic)\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Data tensor shape:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_to_id)) + 1 ) #+1 for zero padding\n",
    "data_1_val = data_1[-1440:] #select 6000 sentences as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch generator to train a neural network\n",
    "\n",
    "For padding sentences to max length we use [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_generator(TRAIN_DATA_FILE, batchsize):\n",
    "    # Create iterator that reads dataset file batch by batch \n",
    "    #your code here\n",
    "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=batchsize, iterator=True)\n",
    "    for df in reader:\n",
    "        # Read a column that contains headlines\n",
    "        headlines = df['headline'].str.strip().tolist()\n",
    "        # Tokenize texts and create padded tensor composed of tokens indexes\n",
    "        sequences = tokenizer.texts_to_sequences(headlines)\n",
    "        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        # Return input-target pairs\n",
    "        yield [data_train, data_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained GloVe vectors described in [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors.\n"
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.50d.txt', encoding='utf-8') as f:\n",
    "    # read rows from file line by line\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # Get word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') # Get elements of word's vector\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrix from embedding vectors. Any row of the matrix is a word's vector. We get words from the dictionary word_to_id defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Null word embeddings: 1\n"
    }
   ],
   "source": [
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM)) # Create empty matrix (max number of tokens, dimension of the embedding vectors)\n",
    "for word, i in word_to_id.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "# compute number of words which there aren't in the GloVe vectors\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 32 # dimensionality of the hidden state in encoder and decoder RNN's\n",
    "intermediate_dim = 96 # dimensionality of variational space into which we map encoder's hidden state\n",
    "epsilon_std = 1.0 # standard deviation of gaussian noise\n",
    "act = ELU() # activation function of projection layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder of the variational autoencoder. It based on bidirectional LSTM\n",
    "\n",
    "We use following layers: [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [ELU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(None, max_len)) # Input layer fo the net. \n",
    "# Write an embedding layer for the input sequences of indexes. \n",
    "# Use pretrained word embeddings as a embedding layer weights and don't update these weights\n",
    "\n",
    "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n",
    "                            input_length=max_len, trainable=False)(x)\n",
    "\n",
    "# Bidirectional LSTM encoder\n",
    "\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "\n",
    "h = Dropout(0.2)(h) # Dropout for the BiLSTM layer to avoid overfitting \n",
    "\n",
    "# Fully-connected layer to map encoder hidden state into variational space\n",
    "\n",
    "h = Dense(intermediate_dim, activation='linear')(h)\n",
    "h = act(h)\n",
    "\n",
    "h = Dropout(0.2)(h) # Dropout for the fully-connected layer to avoid overfitting \n",
    "z_mean = Dense(latent_dim)(h) # Fully-connected layer to map variational space into means space \n",
    "z_log_var = Dense(latent_dim)(h) # Fully-connected layer to map variational space into standard deviations space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mechanism for sampling hidden vectors from variational space\n",
    "\n",
    "To apply it to our model we use [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    # Vectors from means space and standard deviations space respectively\n",
    "    z_mean, z_log_var = args\n",
    "    # Sample random vectors from normal distribution with mean=0 and std=epsilon_std\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    \n",
    "    # Get new hidden state for decoder using vectors from means, standard deviations and normal random spaces\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Get hidden states for the decoder\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define decoder of the autoencoder\n",
    "\n",
    "For this we use following layers: [RepeatVector](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the hidden state vector to form input sequence for decoder\n",
    "repeated_context = RepeatVector(max_len)\n",
    "# Decoder LSTM\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "# Layer for mapping from hidden satates space to the space of dimension equal to size of vocabulary\n",
    "decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))\n",
    "# Generated sequence\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define layer for loss computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_loss(y_true, y_pred):\n",
    "    # Return tensor filled with ones with shape equal generated sequence shape\n",
    "    return K.zeros_like(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        # Create tensor (batch_size, max_sequence_len) filled with ones to consider all elements of generated sequence \n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # Get tensor with similar shape as x\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        # Compute sequence reconstruction loss\n",
    "        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels,\n",
    "                                                    weights=self.target_weights,\n",
    "                                                    average_across_timesteps=False,\n",
    "                                                    average_across_batch=False), axis=-1)\n",
    "        # Compute KL-divergence as Variational loss \n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        # Composite loss (reconstruction loss + Variational loss)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0] # input sequence\n",
    "        x_decoded_mean = inputs[1] # reconstructed sequence\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean) # Compute loss of the model\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape\n",
    "        return K.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the model\n",
    "\n",
    "To define model we use [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) from tensorflow.\n",
    "\n",
    "To train model employ [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(None, 15) (12, 15, 20001)\nModel: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            (None, 15)           0                                            \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, 15, 50)       1000050     input_3[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_3 (Bidirectional) (None, 192)          112896      embedding_3[0][0]                \n__________________________________________________________________________________________________\ndropout_5 (Dropout)             (None, 192)          0           bidirectional_3[0][0]            \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 96)           18528       dropout_5[0][0]                  \n__________________________________________________________________________________________________\nelu_3 (ELU)                     (None, 96)           0           dense_9[0][0]                    \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 96)           0           elu_3[0][0]                      \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 32)           3104        dropout_6[0][0]                  \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 32)           3104        dropout_6[0][0]                  \n__________________________________________________________________________________________________\nlambda_3 (Lambda)               (None, 32)           0           dense_10[0][0]                   \n                                                                 dense_11[0][0]                   \n__________________________________________________________________________________________________\nrepeat_vector_3 (RepeatVector)  (None, 15, 32)       0           lambda_3[0][0]                   \n__________________________________________________________________________________________________\nlstm_6 (LSTM)                   (None, 15, 96)       49536       repeat_vector_3[0][0]            \n__________________________________________________________________________________________________\ntime_distributed_3 (TimeDistrib (None, 15, 20001)    1940097     lstm_6[0][0]                     \n__________________________________________________________________________________________________\ncustom_variational_layer_3 (Cus [(None, 15), (None,  0           input_3[0][0]                    \n                                                                 time_distributed_3[0][0]         \n==================================================================================================\nTotal params: 3,127,315\nTrainable params: 2,127,265\nNon-trainable params: 1,000,050\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# Create custom layer for loss computing\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "\n",
    "vae = Model(x, [loss_layer])\n",
    "# Use Adam optimizer with learning rate = 0.01\n",
    "opt = Adam(lr=0.01)\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])\n",
    "# Show model structure\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint function to save states of our model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(model_name):\n",
    "    filepath = model_name + \".h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        # If directory doesn't exist, create the directory\n",
    "        os.mkdir(directory)\n",
    "    # Save model states\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
    "    return checkpointer\n",
    "\n",
    "# Create model checkpointer\n",
    "checkpointer = create_model_checkpoint(f'{os.path.join(os.getcwd(),\"checkpoints\", \"vae_seq2seq\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model, test model after each apoch and save model's state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "977.0\n"
    }
   ],
   "source": [
    "nb_epoch = 25 # number of epochs for model training\n",
    "n_steps = len(data_1) / batch_size # Number of steps per epoch\n",
    "print(n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------epoch:  0 -------\nEpoch 1/1\n977/977 [==============================] - 365s 374ms/step - loss: 90.3453 - val_loss: 79.3948\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  1 -------\nEpoch 1/1\n977/977 [==============================] - 555s 568ms/step - loss: 78.9957 - val_loss: 77.8978\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  2 -------\nEpoch 1/1\n977/977 [==============================] - 414s 424ms/step - loss: 77.3269 - val_loss: 77.5395\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  3 -------\nEpoch 1/1\n977/977 [==============================] - 441s 451ms/step - loss: 75.7985 - val_loss: 77.9062\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  4 -------\nEpoch 1/1\n977/977 [==============================] - 373s 382ms/step - loss: 73.9712 - val_loss: 78.3960\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  5 -------\nEpoch 1/1\n977/977 [==============================] - 329s 336ms/step - loss: 72.1601 - val_loss: 79.0964\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  6 -------\nEpoch 1/1\n977/977 [==============================] - 293s 300ms/step - loss: 70.6930 - val_loss: 79.9292\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  7 -------\nEpoch 1/1\n977/977 [==============================] - 289s 295ms/step - loss: 69.3078 - val_loss: 80.8582\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n"
    }
   ],
   "source": [
    "for counter in range(nb_epoch):\n",
    "    print('-------epoch: ', counter, '-------')\n",
    "    # Train model. Test and save model after every epoch\n",
    "    vae.fit_generator(sent_generator('Dataset.csv', batch_size),\n",
    "                      steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n",
    "                      validation_data=(data_1_val, data_1_val))\n",
    "vae.save(r'vae_lstmFull32dim96hid.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble encoder and decoder for sentence generation sampled from variational space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make separate encoder to encode input sentence\n",
    "encoder = Model(x, z_mean)\n",
    "# Input layer for decoder to decode vectors sampled from variational space \n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "# Apply LSTM to decode hidden vector into sequence\n",
    "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
    "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "# Apply softmax to get most probable token\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "# Make decoderfor sempled sentences\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "charlize ethics blasts 'peasant 'peasant murky murky 'russia 'russia 'russia 'russia\nold friend avoided in hometown convenience store\n"
    }
   ],
   "source": [
    "# Dictionary maps indexes to words\n",
    "index2word = {v: k for k, v in word_to_id.items()}\n",
    "# Fit sentences from validation set into encoder\n",
    "sent_encoded = encoder.predict(data_1_val, batch_size=16)\n",
    "# Decode encoded sentences\n",
    "x_test_reconstructed = generator.predict(sent_encoded)\n",
    "\n",
    "sent_idx = 400\n",
    "# Get words indexes with highest probability for the 500th sentence from validation set\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
    "# Map indexes of generated sentence to words\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "word_list = ' '.join([w for w in word_list if w])\n",
    "print(word_list)\n",
    "# Map indexes of input sentence to words\n",
    "original_sent = list(np.vectorize(index2word.get)(data_1_val[sent_idx]))\n",
    "original_sent = ' '.join([w for w in original_sent if w])\n",
    "print(original_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('yandex': virtualenv)",
   "language": "python",
   "name": "python37564bityandexvirtualenv005add28e08e42ddb94988cf771fa745"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}