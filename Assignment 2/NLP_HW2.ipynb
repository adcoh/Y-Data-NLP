{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Text generation by Markov chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chain is a probabalistic model in which the probability of each event depends only on the state attained in the previous event. [markovify](https://github.com/jsvine/markovify) is a library for text generation by Markov chain.\n",
    "\n",
    "Use \"pip install markovify\" to install markovify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download [Dataset.csv](https://drive.google.com/file/d/1raxIkJZ4lMTvgTB8eYjxu4Ux8aNL-x0s/view?usp=sharing) composed of sarcastic and serious headlines for the news. The csv-file consists of two columns. \"headline\" column contains texts of headlines. \"is_sarcastic\" column contain 0 if the hiadline is serious and 1 otherwise.\n",
    "\n",
    "Read dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/Sarcasm_Headlines_Dataset.json\", lines=True)\n",
    "df.drop(columns=\"article_link\",inplace=True)\n",
    "df.head(3)\n",
    "df.to_csv(\"Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            headline  is_sarcastic\n0  former versace store clerk sues over secret 'b...             0\n1  the 'roseanne' revival catches up to our thorn...             0\n2  mom starting to fear son's web series closest ...             1\n3  boehner just wants wife to listen, not come up...             1\n4  j.k. rowling wishes snape happy birthday in th...             0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>is_sarcastic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>former versace store clerk sues over secret 'b...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>the 'roseanne' revival catches up to our thorn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mom starting to fear son's web series closest ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>boehner just wants wife to listen, not come up...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>j.k. rowling wishes snape happy birthday in th...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 11724 sarcastic texts in Dataset.csv\nFound 14985 serious texts in Dataset.csv\n"
    }
   ],
   "source": [
    "texts_serious = [] # list of serious hiadline texts\n",
    "texts_sarcastic = [] # list of sarcastic hiadline texts\n",
    "with open('Dataset.csv', encoding='utf-8') as f:\n",
    "    # Rread csv-file by DictReader from csv library\n",
    "    reader = csv.DictReader(f) \n",
    "    for line in reader:\n",
    "        # read texts of headline\n",
    "        headline = line['headline'].strip()\n",
    "        # read sarcasticity of headline\n",
    "        is_sarcastic = int(line['is_sarcastic'].strip())\n",
    "        if is_sarcastic:\n",
    "            texts_sarcastic.append(headline)\n",
    "        else:\n",
    "            texts_serious.append(headline)\n",
    "print('Found {} sarcastic texts in Dataset.csv'.format(len(texts_sarcastic)))\n",
    "print('Found {} serious texts in Dataset.csv'.format(len(texts_serious)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge headlines into one text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_0 = '\\n'.join(texts_serious)\n",
    "text_1 = '\\n'.join(texts_sarcastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Markov chain model for serious headlines generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "serious_model = markovify.NewlineText(text_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 serious headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "activists rally behind pope's message on climate, world bank says\nobamacare benefits plenty of people will be in poverty by 2030 without action on climate, world bank accidentally left me a voicemail discussing their strategy to downplay rights abuses in yemen\nthe best way to know about louis c.k. comedy special\nchina excludes same-sex couples at center of supreme court hears case against shakira\nis this the best taxi and ride-hailing apps around the world that caring for every preemie, every day\nwhy we're lucky if we were all family generation changers?\npaul ryan's wonk shtick is getting her own way in paris: meet author/publisher adria j. cimino\nshredding the past that are easily available in the classroom with braille valentines\n15 pieces of feminist protestors celebrate the history of bigotry\njohn lewis won't attend civil rights lawyer decided to attach my business to south carolina, where the money pledged to fight later\nwhy america is globally shamed for wearing turbans to nfl game\nwhy successful ceos must think like the cure and grimes shining at bestival toronto, it was over before donald trump's inauguration\nwho needs 10 fps when you truly want to be?\nlauren conrad is a liar?\nu.s. must unite on gun control as it struggles to pick up the race line: what i want to sell 6,000 boxes of cookies in nyc\nsyria: refugee communities and to our national parks. will you?\nwill smith and gabrielle union pulls a u-turn, decides tipping is ok with banning gymnastics leotards to prevent attacks\nhow nebraska can return to the circus\nmassachusetts is offering $49 flights in an ultra-orthodox community\ncreepy photos of abandoned insane asylums will keep events out of cockpit before alps crash\n"
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(serious_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model and generate 20 sarcastic headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "fucking oasis to probably be nice having friends\nnew roommates attempt to increase recommended dosage of acetaminophen for children who can lift 27 pounds with tongue to editor-in-chief\nuniversity with 20,000 applicants to choose how he's covered by freelancers union\nchiropractor scrambling to reestablish whereabouts of man being dragged behind boat\nbonobo embarrassed after realizing he might be giants behind the music episode lacks sex, drugs\ngm covered with giant tarp until it has no idea which friend going to take on airport gate agent\nmom's bathing suit has no idea what's going on that day\nnew department of interior brings down yet another failed business\nj.k. rowling revealed to be sex partner\nulta releases line of lizards winding out door outside national geographic finally captures rare shot of antelopeater feeding\nwe in golden age of assistant managers\narticle about one day\ncompany's employees spend entire life\ngaunt, sickly kirby takes leave of absence from video games seemed like solution to immigration, unemployment, crime problems\nrookie justice gorsuch assigned to supreme court told to take it for the 2020 nomination\ncouple always dreamed of fighting in afghanistan\npope francis concerned about life insurance\nsat found to contain oil spill disaster zone\ntriumph of human condition\nsports journalist told to leave americans with fewer places to buy toaster back\n"
    }
   ],
   "source": [
    "sarcastic_model = markovify.NewlineText(text_1)\n",
    "for i in range(20):\n",
    "    print(sarcastic_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation by Variation Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of tutorial based on [Text generation with a Variational Autoencoder](https://nicgian.github.io/text-generation-vae/) article. For sentence generation we use Variational Autoencoder (VAE) neural network model that is an extension seq2seq model. Originally VAE was described in [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf) paper. The idea behind Variational Autoencoder is that we impose predefined disribution (e.g., normal distribution) on the latent state formed by encoder. On the one hand this restriction alow us to sample random vectors from normal distribution and generate arbitrary sentences. On the othe hand this restriction form very dense well differentiated space without holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "from keras.layers import Bidirectional, Dense, Embedding, \\\n",
    "Input, Lambda, LSTM, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import random\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dowload [GloVe](http://nlp.stanford.edu/data/glove.6B.zip) pretrained word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip glove.6b.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 15 # Max text length in tokens\n",
    "MAX_NB_WORDS = 20000 # Max words in dictionary\n",
    "EMBEDDING_DIM = 200 # Dimensionality of GloVe vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sentence tokenizer and two dictionaries: word_to_id and id_to_word\n",
    "\n",
    "For tokenisation we use [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) from keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Corpus has 29656 unique tokens\n"
    }
   ],
   "source": [
    "texts = texts_serious + texts_sarcastic\n",
    "tokenizer = Tokenizer(MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_to_id = tokenizer.word_index \n",
    "id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "print(f'Corpus has {len(word_to_id)} unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sarcastic texts and create tensor composed of tokens indexes. If a sentence shorter than MAX_SEQUENCE_LENGTH we pad it. If a sentence longer than MAX_SEQUENCE_LENGTH we cut it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data tensor shape: (11724, 15)\n"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts_sarcastic)\n",
    "data_1 = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Data tensor shape:', data_1.shape)\n",
    "NB_WORDS = (min(tokenizer.num_words, len(word_to_id)) + 1 ) #+1 for zero padding\n",
    "data_1_val = data_1[-1440:] #select 6000 sentences as validation data\n",
    "data_1 = data_1[:-1440]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch generator to train a neural network\n",
    "\n",
    "For padding sentences to max length we use [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_generator(TRAIN_DATA_FILE, batchsize):\n",
    "    # Create iterator that reads dataset file batch by batch \n",
    "    #your code here\n",
    "    reader = pd.read_csv(TRAIN_DATA_FILE, chunksize=batchsize, iterator=True)\n",
    "    for df in reader:\n",
    "        # Read a column that contains headlines\n",
    "        headlines = df['headline'].str.strip().tolist()\n",
    "        # Tokenize texts and create padded tensor composed of tokens indexes\n",
    "        sequences = tokenizer.texts_to_sequences(headlines)\n",
    "        data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        # Return input-target pairs\n",
    "        yield [data_train, data_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained GloVe vectors described in [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 400000 word vectors.\n"
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.200d.txt', encoding='utf-8') as f:\n",
    "    # read rows from file line by line\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0] # Get word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') # Get elements of word's vector\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrix from embedding vectors. Any row of the matrix is a word's vector. We get words from the dictionary word_to_id defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Null word embeddings: 1\n"
    }
   ],
   "source": [
    "glove_embedding_matrix = np.zeros((NB_WORDS, EMBEDDING_DIM)) # Create empty matrix (max number of tokens, dimension of the embedding vectors)\n",
    "for word, i in word_to_id.items():\n",
    "    if i < NB_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be the word embedding of 'unk'.\n",
    "            glove_embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            glove_embedding_matrix[i] = embeddings_index.get('unk')\n",
    "# compute number of words which there aren't in the GloVe vectors\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(glove_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "max_len = MAX_SEQUENCE_LENGTH\n",
    "emb_dim = EMBEDDING_DIM\n",
    "latent_dim = 32 # dimensionality of the hidden state in encoder and decoder RNN's\n",
    "intermediate_dim = 96 # dimensionality of variational space into which we map encoder's hidden state\n",
    "epsilon_std = 1.0 # standard deviation of gaussian noise\n",
    "act = ELU() # activation function of projection layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder of the variational autoencoder. It based on bidirectional LSTM\n",
    "\n",
    "We use following layers: [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), [Bidirectional](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [ELU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(batch_shape=(None, max_len)) # Input layer fo the net. \n",
    "# Write an embedding layer for the input sequences of indexes. \n",
    "# Use pretrained word embeddings as a embedding layer weights and don't update these weights\n",
    "\n",
    "x_embed = Embedding(NB_WORDS, emb_dim, weights=[glove_embedding_matrix],\n",
    "                            input_length=max_len, trainable=False)(x)\n",
    "\n",
    "# Bidirectional LSTM encoder\n",
    "\n",
    "h = Bidirectional(LSTM(intermediate_dim, return_sequences=False, recurrent_dropout=0.2), merge_mode='concat')(x_embed)\n",
    "\n",
    "h = Dropout(0.2)(h) # Dropout for the BiLSTM layer to avoid overfitting \n",
    "\n",
    "# Fully-connected layer to map encoder hidden state into variational space\n",
    "\n",
    "h = Dense(intermediate_dim, activation='linear')(h)\n",
    "h = act(h)\n",
    "\n",
    "h = Dropout(0.2)(h) # Dropout for the fully-connected layer to avoid overfitting \n",
    "z_mean = Dense(latent_dim)(h) # Fully-connected layer to map variational space into means space \n",
    "z_log_var = Dense(latent_dim)(h) # Fully-connected layer to map variational space into standard deviations space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mechanism for sampling hidden vectors from variational space\n",
    "\n",
    "To apply it to our model we use [Lambda](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    # Vectors from means space and standard deviations space respectively\n",
    "    z_mean, z_log_var = args\n",
    "    # Sample random vectors from normal distribution with mean=0 and std=epsilon_std\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    \n",
    "    # Get new hidden state for decoder using vectors from means, standard deviations and normal random spaces\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Get hidden states for the decoder\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define decoder of the autoencoder\n",
    "\n",
    "For this we use following layers: [RepeatVector](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RepeatVector), [LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM), [TimeDistributed](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed), [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the hidden state vector to form input sequence for decoder\n",
    "repeated_context = RepeatVector(max_len)\n",
    "# Decoder LSTM\n",
    "decoder_h = LSTM(intermediate_dim, return_sequences=True, recurrent_dropout=0.2)\n",
    "# Layer for mapping from hidden satates space to the space of dimension equal to size of vocabulary\n",
    "decoder_mean = TimeDistributed(Dense(NB_WORDS, activation='linear'))\n",
    "# Generated sequence\n",
    "h_decoded = decoder_h(repeated_context(z))\n",
    "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define layer for loss computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_loss(y_true, y_pred):\n",
    "    # Return tensor filled with ones with shape equal generated sequence shape\n",
    "    return K.zeros_like(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "        # Create tensor (batch_size, max_sequence_len) filled with ones to consider all elements of generated sequence \n",
    "        self.target_weights = tf.constant(np.ones((batch_size, max_len)), tf.float32)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean):\n",
    "        # Get tensor with similar shape as x\n",
    "        labels = tf.cast(x, tf.int32)\n",
    "        # Compute sequence reconstruction loss\n",
    "        xent_loss = K.sum(tfa.seq2seq.sequence_loss(x_decoded_mean, labels,\n",
    "                                                    weights=self.target_weights,\n",
    "                                                    average_across_timesteps=False,\n",
    "                                                    average_across_batch=False), axis=-1)\n",
    "        # Compute KL-divergence as Variational loss \n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        # Composite loss (reconstruction loss + Variational loss)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0] # input sequence\n",
    "        x_decoded_mean = inputs[1] # reconstructed sequence\n",
    "        print(x.shape, x_decoded_mean.shape)\n",
    "        loss = self.vae_loss(x, x_decoded_mean) # Compute loss of the model\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # we don't use this output, but it has to have the correct shape\n",
    "        return K.ones_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble the model\n",
    "\n",
    "To define model we use [Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) from tensorflow.\n",
    "\n",
    "To train model employ [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(None, 15) (12, 15, 20001)\nModel: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 15)           0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 15, 200)      4000200     input_1[0][0]                    \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 192)          228096      embedding_1[0][0]                \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 192)          0           bidirectional_1[0][0]            \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 96)           18528       dropout_1[0][0]                  \n__________________________________________________________________________________________________\nelu_1 (ELU)                     (None, 96)           0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 96)           0           elu_1[0][0]                      \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 32)           3104        dropout_2[0][0]                  \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 32)           3104        dropout_2[0][0]                  \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 32)           0           dense_2[0][0]                    \n                                                                 dense_3[0][0]                    \n__________________________________________________________________________________________________\nrepeat_vector_1 (RepeatVector)  (None, 15, 32)       0           lambda_1[0][0]                   \n__________________________________________________________________________________________________\nlstm_2 (LSTM)                   (None, 15, 96)       49536       repeat_vector_1[0][0]            \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 15, 20001)    1940097     lstm_2[0][0]                     \n__________________________________________________________________________________________________\ncustom_variational_layer_1 (Cus [(None, 15), (None,  0           input_1[0][0]                    \n                                                                 time_distributed_1[0][0]         \n==================================================================================================\nTotal params: 6,242,665\nTrainable params: 2,242,465\nNon-trainable params: 4,000,200\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# Create custom layer for loss computing\n",
    "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
    "\n",
    "vae = Model(x, [loss_layer])\n",
    "# Use Adam optimizer with learning rate = 0.01\n",
    "opt = Adam(lr=0.01)\n",
    "vae.compile(optimizer='adam', loss=[zero_loss])\n",
    "# Show model structure\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint function to save states of our model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_checkpoint(model_name):\n",
    "    filepath = model_name + \".h5\"\n",
    "    directory = os.path.dirname(filepath)\n",
    "    try:\n",
    "        # Check if directory exists\n",
    "        os.stat(directory)\n",
    "    except:\n",
    "        # If directory doesn't exist, create the directory\n",
    "        os.mkdir(directory)\n",
    "    # Save model states\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, verbose=1, save_best_only=False)\n",
    "    return checkpointer\n",
    "\n",
    "# Create model checkpointer\n",
    "checkpointer = create_model_checkpoint(f'{os.path.join(os.getcwd(),\"checkpoints\", \"vae_seq2seq\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model, test model after each apoch and save model's state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "857.0\n"
    }
   ],
   "source": [
    "nb_epoch = 50 # number of epochs for model training\n",
    "n_steps = len(data_1) / batch_size # Number of steps per epoch\n",
    "print(n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------epoch:  0 -------\nEpoch 1/1\n857/857 [==============================] - 378s 441ms/step - loss: 59.4132 - val_loss: 93.6342\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  1 -------\nEpoch 1/1\n857/857 [==============================] - 337s 393ms/step - loss: 59.1343 - val_loss: 94.1721\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  2 -------\nEpoch 1/1\n857/857 [==============================] - 296s 346ms/step - loss: 58.7743 - val_loss: 94.5053\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  3 -------\nEpoch 1/1\n857/857 [==============================] - 296s 345ms/step - loss: 58.5282 - val_loss: 94.8363\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  4 -------\nEpoch 1/1\n857/857 [==============================] - 267s 312ms/step - loss: 58.3094 - val_loss: 95.4909\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  5 -------\nEpoch 1/1\n857/857 [==============================] - 278s 325ms/step - loss: 57.9879 - val_loss: 95.7281\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  6 -------\nEpoch 1/1\n857/857 [==============================] - 210s 245ms/step - loss: 57.7524 - val_loss: 96.0252\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  7 -------\nEpoch 1/1\n857/857 [==============================] - 188s 219ms/step - loss: 57.5089 - val_loss: 96.3528\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  8 -------\nEpoch 1/1\n857/857 [==============================] - 183s 213ms/step - loss: 57.2281 - val_loss: 96.7459\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  9 -------\nEpoch 1/1\n857/857 [==============================] - 182s 212ms/step - loss: 57.0960 - val_loss: 96.6093\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  10 -------\nEpoch 1/1\n857/857 [==============================] - 188s 219ms/step - loss: 56.8878 - val_loss: 96.8712\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  11 -------\nEpoch 1/1\n857/857 [==============================] - 186s 217ms/step - loss: 56.7098 - val_loss: 97.4123\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  12 -------\nEpoch 1/1\n857/857 [==============================] - 182s 212ms/step - loss: 56.5410 - val_loss: 97.4738\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  13 -------\nEpoch 1/1\n857/857 [==============================] - 184s 215ms/step - loss: 56.3483 - val_loss: 98.0235\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  14 -------\nEpoch 1/1\n857/857 [==============================] - 182s 213ms/step - loss: 56.1823 - val_loss: 98.0389\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  15 -------\nEpoch 1/1\n857/857 [==============================] - 183s 213ms/step - loss: 55.9234 - val_loss: 98.4755\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  16 -------\nEpoch 1/1\n857/857 [==============================] - 181s 211ms/step - loss: 55.6920 - val_loss: 98.6997\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  17 -------\nEpoch 1/1\n857/857 [==============================] - 187s 219ms/step - loss: 55.5847 - val_loss: 99.1879\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  18 -------\nEpoch 1/1\n857/857 [==============================] - 188s 219ms/step - loss: 55.4283 - val_loss: 99.1795\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  19 -------\nEpoch 1/1\n857/857 [==============================] - 179s 209ms/step - loss: 55.2266 - val_loss: 99.4615\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  20 -------\nEpoch 1/1\n857/857 [==============================] - 177s 207ms/step - loss: 55.1231 - val_loss: 99.6483\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  21 -------\nEpoch 1/1\n857/857 [==============================] - 190s 222ms/step - loss: 55.0389 - val_loss: 99.9346\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  22 -------\nEpoch 1/1\n857/857 [==============================] - 187s 218ms/step - loss: 54.8485 - val_loss: 100.2551\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  23 -------\nEpoch 1/1\n857/857 [==============================] - 197s 230ms/step - loss: 54.7784 - val_loss: 100.2512\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  24 -------\nEpoch 1/1\n857/857 [==============================] - 195s 228ms/step - loss: 54.6378 - val_loss: 100.4602\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  25 -------\nEpoch 1/1\n857/857 [==============================] - 192s 225ms/step - loss: 54.4368 - val_loss: 100.6687\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  26 -------\nEpoch 1/1\n857/857 [==============================] - 180s 210ms/step - loss: 54.4222 - val_loss: 100.6791\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  27 -------\nEpoch 1/1\n857/857 [==============================] - 193s 225ms/step - loss: 54.3266 - val_loss: 100.7003\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  28 -------\nEpoch 1/1\n857/857 [==============================] - 186s 217ms/step - loss: 54.1245 - val_loss: 100.9657\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  29 -------\nEpoch 1/1\n857/857 [==============================] - 188s 219ms/step - loss: 54.0412 - val_loss: 101.1249\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  30 -------\nEpoch 1/1\n857/857 [==============================] - 203s 237ms/step - loss: 53.9275 - val_loss: 101.5073\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  31 -------\nEpoch 1/1\n857/857 [==============================] - 189s 220ms/step - loss: 53.7916 - val_loss: 101.7510\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  32 -------\nEpoch 1/1\n857/857 [==============================] - 180s 210ms/step - loss: 53.7222 - val_loss: 101.6440\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  33 -------\nEpoch 1/1\n857/857 [==============================] - 183s 214ms/step - loss: 53.6465 - val_loss: 101.9835\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  34 -------\nEpoch 1/1\n857/857 [==============================] - 184s 215ms/step - loss: 53.5848 - val_loss: 102.0518\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  35 -------\nEpoch 1/1\n857/857 [==============================] - 185s 215ms/step - loss: 53.4396 - val_loss: 102.7012\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  36 -------\nEpoch 1/1\n857/857 [==============================] - 183s 213ms/step - loss: 53.4234 - val_loss: 102.5050\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  37 -------\nEpoch 1/1\n857/857 [==============================] - 185s 215ms/step - loss: 53.3404 - val_loss: 102.6220\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  38 -------\nEpoch 1/1\n857/857 [==============================] - 303s 353ms/step - loss: 53.1569 - val_loss: 103.2666\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  39 -------\nEpoch 1/1\n857/857 [==============================] - 309s 361ms/step - loss: 53.0960 - val_loss: 102.9292\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  40 -------\nEpoch 1/1\n857/857 [==============================] - 296s 345ms/step - loss: 53.0439 - val_loss: 103.2700\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  41 -------\nEpoch 1/1\n857/857 [==============================] - 306s 357ms/step - loss: 52.9862 - val_loss: 103.1914\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  42 -------\nEpoch 1/1\n857/857 [==============================] - 248s 289ms/step - loss: 52.8875 - val_loss: 103.4953\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  43 -------\nEpoch 1/1\n857/857 [==============================] - 238s 278ms/step - loss: 52.7756 - val_loss: 103.7300\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  44 -------\nEpoch 1/1\n857/857 [==============================] - 240s 280ms/step - loss: 52.7948 - val_loss: 103.5257\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  45 -------\nEpoch 1/1\n857/857 [==============================] - 340s 397ms/step - loss: 52.7043 - val_loss: 103.3726\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  46 -------\nEpoch 1/1\n857/857 [==============================] - 347s 405ms/step - loss: 52.6596 - val_loss: 103.6349\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  47 -------\nEpoch 1/1\n857/857 [==============================] - 330s 385ms/step - loss: 52.7502 - val_loss: 103.8869\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  48 -------\nEpoch 1/1\n857/857 [==============================] - 336s 392ms/step - loss: 52.6485 - val_loss: 103.9974\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n-------epoch:  49 -------\nEpoch 1/1\n857/857 [==============================] - 244s 284ms/step - loss: 52.5947 - val_loss: 103.6644\n\nEpoch 00001: saving model to /Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 2/checkpoints/vae_seq2seq.h5\n"
    }
   ],
   "source": [
    "for counter in range(nb_epoch):\n",
    "    print('-------epoch: ', counter, '-------')\n",
    "    # Train model. Test and save model after every epoch\n",
    "    vae.fit_generator(sent_generator('Dataset.csv', batch_size),\n",
    "                      steps_per_epoch=n_steps, epochs=1, callbacks=[checkpointer],\n",
    "                      validation_data=(data_1_val, data_1_val))\n",
    "vae.save(r'vae_lstmFull32dim96hid.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assemble encoder and decoder for sentence generation sampled from variational space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make separate encoder to encode input sentence\n",
    "encoder = Model(x, z_mean)\n",
    "# Input layer for decoder to decode vectors sampled from variational space \n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "# Apply LSTM to decode hidden vector into sequence\n",
    "_h_decoded = decoder_h(repeated_context(decoder_input))\n",
    "# Decode every time step vector of the decoded sequence into space of dimension equal to size of vocabulary\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "# Apply softmax to get most probable token\n",
    "_x_decoded_mean = Activation('softmax')(_x_decoded_mean)\n",
    "# Make decoderfor sempled sentences\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "of of of overnight oliver oliver\nold friend avoided in hometown convenience store\n"
    }
   ],
   "source": [
    "# Dictionary maps indexes to words\n",
    "index2word = {v: k for k, v in word_to_id.items()}\n",
    "# Fit sentences from validation set into encoder\n",
    "sent_encoded = encoder.predict(data_1_val, batch_size=16)\n",
    "# Decode encoded sentences\n",
    "x_test_reconstructed = generator.predict(sent_encoded)\n",
    "\n",
    "sent_idx = 400\n",
    "# Get words indexes with highest probability for the 500th sentence from validation set\n",
    "reconstructed_indexes = np.apply_along_axis(np.argmax, 1, x_test_reconstructed[sent_idx])\n",
    "# Map indexes of generated sentence to words\n",
    "word_list = list(np.vectorize(index2word.get)(reconstructed_indexes))\n",
    "word_list = ' '.join([w for w in word_list if w])\n",
    "print(word_list)\n",
    "# Map indexes of input sentence to words\n",
    "original_sent = list(np.vectorize(index2word.get)(data_1_val[sent_idx]))\n",
    "original_sent = ' '.join([w for w in original_sent if w])\n",
    "print(original_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('yandex': virtualenv)",
   "language": "python",
   "name": "python37564bityandexvirtualenv005add28e08e42ddb94988cf771fa745"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}