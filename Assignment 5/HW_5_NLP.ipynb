{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qdV92T_1GUVN"
   },
   "source": [
    "import numpy as np\n",
    "# ^^^ pyforest auto-imports - don't write above this line\n",
    "# HW5 - Rating prediction using Amazon's Reviews\n",
    "    \n",
    "In this exercise, you'll train a text classification on a **subset** of the the Amazon's Reviews dataset. \n",
    "\n",
    "The Amazon's Reviews dataset  contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\n",
    "\n",
    "\n",
    "We will focus on the Home and Kitchen segment which contains ~550k reviews and can be downloaded here: http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz\n",
    "\n",
    "You will predict the rating that was given to a product from the review.\n",
    "\n",
    "The dataset contains the following fields for each review, in JSON format:\n",
    "1. \"reviewerID\": \"A11N155CW1UV02\",\n",
    "1. \"asin\": \"B000H00VBQ\",\n",
    "1. \"reviewerName\": \"AdrianaM\"\n",
    "1. \"helpful\": [0, 0]\n",
    "1. \"reviewText\": \"I had big expectations because I love English TV, in particular Investigative and detective stuff but this guy is really boring. It didn't appeal to me at all.\"\n",
    "1. \"overall\": 2.0\n",
    "1. \"summary\": \"A little bit boring for me\"\n",
    "1. \"unixReviewTime\": 1399075200\n",
    "1. \"reviewTime\": \"05 3, 2014\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Please note that the **only** two fields that you are allowed to use in this exercise are \"reviewText\" which contains the review and \"overall\" which contains the rating. Other than that you have the **option** to use the \"asin\" field which is a unique product identifier. You may (or may not :) ) find this field useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -nc http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vH0ExGViGUWC"
   },
   "source": [
    "## General guidelines\n",
    "\n",
    "1. You are required to implement at least two models.\n",
    "1. The first should be a CNN or an RNN (or a combination) and should include the use of Glove embeddings.\n",
    "1. The second model should be implemented using the transformers package and include Transfer learning concepts that were mentioned in the Lecture.\n",
    "1. Pay attention to any preprocessing steps that are needed.\n",
    "1. Feel free to be creative and use any method which was mentioned in the lectures (e.g., tf-idf, pos,...) extra points will be given to creativity.\n",
    "1. The main criteria for evaluation is not the over-all score but rather the entire process (preprocessing, efficient training ...)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.082752</td>\n",
       "      <td>0.672040</td>\n",
       "      <td>-0.14987</td>\n",
       "      <td>-0.064983</td>\n",
       "      <td>0.056491</td>\n",
       "      <td>0.402280</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>-0.331100</td>\n",
       "      <td>-0.306910</td>\n",
       "      <td>2.0817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14331</td>\n",
       "      <td>0.018267</td>\n",
       "      <td>-0.18643</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>-0.355980</td>\n",
       "      <td>0.053380</td>\n",
       "      <td>-0.050821</td>\n",
       "      <td>-0.191800</td>\n",
       "      <td>-0.378460</td>\n",
       "      <td>-0.06589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.207510</td>\n",
       "      <td>-0.12578</td>\n",
       "      <td>-0.593250</td>\n",
       "      <td>0.125250</td>\n",
       "      <td>0.159750</td>\n",
       "      <td>0.137480</td>\n",
       "      <td>-0.331570</td>\n",
       "      <td>-0.136940</td>\n",
       "      <td>1.7893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16165</td>\n",
       "      <td>-0.066737</td>\n",
       "      <td>-0.29556</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>-0.281350</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.140190</td>\n",
       "      <td>0.138710</td>\n",
       "      <td>-0.360490</td>\n",
       "      <td>-0.03500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.272040</td>\n",
       "      <td>-0.062030</td>\n",
       "      <td>-0.18840</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>-0.018158</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>-0.138770</td>\n",
       "      <td>0.177080</td>\n",
       "      <td>0.177090</td>\n",
       "      <td>2.5882</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.42810</td>\n",
       "      <td>0.168990</td>\n",
       "      <td>0.22511</td>\n",
       "      <td>-0.285570</td>\n",
       "      <td>-0.102800</td>\n",
       "      <td>-0.018168</td>\n",
       "      <td>0.114070</td>\n",
       "      <td>0.130150</td>\n",
       "      <td>-0.183170</td>\n",
       "      <td>0.13230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.185670</td>\n",
       "      <td>0.066008</td>\n",
       "      <td>-0.25209</td>\n",
       "      <td>-0.117250</td>\n",
       "      <td>0.265130</td>\n",
       "      <td>0.064908</td>\n",
       "      <td>0.122910</td>\n",
       "      <td>-0.093979</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>2.4926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59396</td>\n",
       "      <td>-0.097729</td>\n",
       "      <td>0.20072</td>\n",
       "      <td>0.170550</td>\n",
       "      <td>-0.004736</td>\n",
       "      <td>-0.039709</td>\n",
       "      <td>0.324980</td>\n",
       "      <td>-0.023452</td>\n",
       "      <td>0.123020</td>\n",
       "      <td>0.33120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.319240</td>\n",
       "      <td>0.063160</td>\n",
       "      <td>-0.27858</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.079248</td>\n",
       "      <td>-0.214620</td>\n",
       "      <td>-0.104950</td>\n",
       "      <td>0.154950</td>\n",
       "      <td>-0.033530</td>\n",
       "      <td>2.4834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.12977</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.18888</td>\n",
       "      <td>-0.004274</td>\n",
       "      <td>-0.106450</td>\n",
       "      <td>-0.258100</td>\n",
       "      <td>-0.044629</td>\n",
       "      <td>0.082745</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.25045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2        3         4         5         6         7    \\\n",
       "0                                                                          \n",
       ",   -0.082752  0.672040 -0.14987 -0.064983  0.056491  0.402280  0.002775   \n",
       ".    0.012001  0.207510 -0.12578 -0.593250  0.125250  0.159750  0.137480   \n",
       "the  0.272040 -0.062030 -0.18840  0.023225 -0.018158  0.006719 -0.138770   \n",
       "and -0.185670  0.066008 -0.25209 -0.117250  0.265130  0.064908  0.122910   \n",
       "to   0.319240  0.063160 -0.27858  0.261200  0.079248 -0.214620 -0.104950   \n",
       "\n",
       "          8         9       10   ...      291       292      293       294  \\\n",
       "0                                ...                                         \n",
       ",   -0.331100 -0.306910  2.0817  ... -0.14331  0.018267 -0.18643  0.207090   \n",
       ".   -0.331570 -0.136940  1.7893  ...  0.16165 -0.066737 -0.29556  0.022612   \n",
       "the  0.177080  0.177090  2.5882  ... -0.42810  0.168990  0.22511 -0.285570   \n",
       "and -0.093979  0.024321  2.4926  ... -0.59396 -0.097729  0.20072  0.170550   \n",
       "to   0.154950 -0.033530  2.4834  ... -0.12977  0.371300  0.18888 -0.004274   \n",
       "\n",
       "          295       296       297       298       299      300  \n",
       "0                                                               \n",
       ",   -0.355980  0.053380 -0.050821 -0.191800 -0.378460 -0.06589  \n",
       ".   -0.281350  0.063500  0.140190  0.138710 -0.360490 -0.03500  \n",
       "the -0.102800 -0.018168  0.114070  0.130150 -0.183170  0.13230  \n",
       "and -0.004736 -0.039709  0.324980 -0.023452  0.123020  0.33120  \n",
       "to  -0.106450 -0.258100 -0.044629  0.082745  0.097801  0.25045  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = zipfile.ZipFile(\"/Users/Adam/workspace/yandex/Y-Data/2nd Semester/NLP/Assignment 4/glove.840B.300d.zip\")\n",
    "glove_pd = pd.read_csv(z.open('glove.840B.300d.txt'), sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNK</th>\n",
       "      <td>0.22418134</td>\n",
       "      <td>-0.28881392</td>\n",
       "      <td>0.13854356</td>\n",
       "      <td>0.00365387</td>\n",
       "      <td>-0.12870757</td>\n",
       "      <td>0.10243822</td>\n",
       "      <td>0.061626635</td>\n",
       "      <td>0.07318011</td>\n",
       "      <td>-0.061350107</td>\n",
       "      <td>-1.3477012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.10653763</td>\n",
       "      <td>-0.06199595</td>\n",
       "      <td>0.028849555</td>\n",
       "      <td>0.03230154</td>\n",
       "      <td>0.023856193</td>\n",
       "      <td>0.069950655</td>\n",
       "      <td>0.19310954</td>\n",
       "      <td>-0.077677034</td>\n",
       "      <td>-0.144811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.082752</td>\n",
       "      <td>0.67204</td>\n",
       "      <td>-0.14987</td>\n",
       "      <td>-0.064983</td>\n",
       "      <td>0.056491</td>\n",
       "      <td>0.40228</td>\n",
       "      <td>0.0027747</td>\n",
       "      <td>-0.3311</td>\n",
       "      <td>-0.30691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.14331</td>\n",
       "      <td>0.018267</td>\n",
       "      <td>-0.18643</td>\n",
       "      <td>0.20709</td>\n",
       "      <td>-0.35598</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>-0.050821</td>\n",
       "      <td>-0.1918</td>\n",
       "      <td>-0.37846</td>\n",
       "      <td>-0.06589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.20751</td>\n",
       "      <td>-0.12578</td>\n",
       "      <td>-0.59325</td>\n",
       "      <td>0.12525</td>\n",
       "      <td>0.15975</td>\n",
       "      <td>0.13748</td>\n",
       "      <td>-0.33157</td>\n",
       "      <td>-0.13694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.16165</td>\n",
       "      <td>-0.066737</td>\n",
       "      <td>-0.29556</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>-0.28135</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.14019</td>\n",
       "      <td>0.13871</td>\n",
       "      <td>-0.36049</td>\n",
       "      <td>-0.03500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.27204</td>\n",
       "      <td>-0.06203</td>\n",
       "      <td>-0.1884</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>-0.018158</td>\n",
       "      <td>0.0067192</td>\n",
       "      <td>-0.13877</td>\n",
       "      <td>0.17708</td>\n",
       "      <td>0.17709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4281</td>\n",
       "      <td>0.16899</td>\n",
       "      <td>0.22511</td>\n",
       "      <td>-0.28557</td>\n",
       "      <td>-0.1028</td>\n",
       "      <td>-0.018168</td>\n",
       "      <td>0.11407</td>\n",
       "      <td>0.13015</td>\n",
       "      <td>-0.18317</td>\n",
       "      <td>0.13230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0            1           2           3            4           5    \\\n",
       "              0            0           0           0            0           0   \n",
       "UNK  0.22418134  -0.28881392  0.13854356  0.00365387  -0.12870757  0.10243822   \n",
       ",           NaN    -0.082752     0.67204    -0.14987    -0.064983    0.056491   \n",
       ".           NaN     0.012001     0.20751    -0.12578     -0.59325     0.12525   \n",
       "the         NaN      0.27204    -0.06203     -0.1884     0.023225   -0.018158   \n",
       "\n",
       "             6           7             8           9    ...          291  \\\n",
       "               0           0             0           0  ...            0   \n",
       "UNK  0.061626635  0.07318011  -0.061350107  -1.3477012  ...  -0.10653763   \n",
       ",        0.40228   0.0027747       -0.3311    -0.30691  ...     -0.14331   \n",
       ".        0.15975     0.13748      -0.33157    -0.13694  ...      0.16165   \n",
       "the    0.0067192    -0.13877       0.17708     0.17709  ...      -0.4281   \n",
       "\n",
       "             292          293         294          295          296  \\\n",
       "               0            0           0            0            0   \n",
       "UNK  -0.06199595  0.028849555  0.03230154  0.023856193  0.069950655   \n",
       ",       0.018267     -0.18643     0.20709     -0.35598      0.05338   \n",
       ".      -0.066737     -0.29556    0.022612     -0.28135       0.0635   \n",
       "the      0.16899      0.22511    -0.28557      -0.1028    -0.018168   \n",
       "\n",
       "            297           298        299      300  \n",
       "              0             0          0      NaN  \n",
       "UNK  0.19310954  -0.077677034  -0.144811      NaN  \n",
       ",     -0.050821       -0.1918   -0.37846 -0.06589  \n",
       ".       0.14019       0.13871   -0.36049 -0.03500  \n",
       "the     0.11407       0.13015   -0.18317  0.13230  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_string = '0.22418134 -0.28881392 0.13854356 0.00365387 -0.12870757 0.10243822 0.061626635 0.07318011 -0.061350107 -1.3477012 0.42037755 -0.063593924 -0.09683349 0.18086134 0.23704372 0.014126852 0.170096 -1.1491593 0.31497982 0.06622181 0.024687296 0.076693475 0.13851812 0.021302193 -0.06640582 -0.010336159 0.13523154 -0.042144544 -0.11938788 0.006948221 0.13333307 -0.18276379 0.052385733 0.008943111 -0.23957317 0.08500333 -0.006894406 0.0015864656 0.063391194 0.19177166 -0.13113557 -0.11295479 -0.14276934 0.03413971 -0.034278486 -0.051366422 0.18891625 -0.16673574 -0.057783455 0.036823478 0.08078679 0.022949161 0.033298038 0.011784158 0.05643189 -0.042776518 0.011959623 0.011552498 -0.0007971594 0.11300405 -0.031369694 -0.0061559738 -0.009043574 -0.415336 -0.18870236 0.13708843 0.005911723 -0.113035575 -0.030096142 -0.23908928 -0.05354085 -0.044904727 -0.20228513 0.0065645403 -0.09578946 -0.07391877 -0.06487607 0.111740574 -0.048649278 -0.16565254 -0.052037314 -0.078968436 0.13684988 0.0757494 -0.006275573 0.28693774 0.52017444 -0.0877165 -0.33010918 -0.1359622 0.114895485 -0.09744406 0.06269521 0.12118575 -0.08026362 0.35256687 -0.060017522 -0.04889904 -0.06828978 0.088740796 0.003964443 -0.0766291 0.1263925 0.07809314 -0.023164088 -0.5680669 -0.037892066 -0.1350967 -0.11351585 -0.111434504 -0.0905027 0.25174105 -0.14841858 0.034635577 -0.07334565 0.06320108 -0.038343467 -0.05413284 0.042197507 -0.090380974 -0.070528865 -0.009174437 0.009069661 0.1405178 0.02958134 -0.036431845 -0.08625681 0.042951006 0.08230793 0.0903314 -0.12279937 -0.013899368 0.048119213 0.08678239 -0.14450377 -0.04424887 0.018319942 0.015026873 -0.100526 0.06021201 0.74059093 -0.0016333034 -0.24960588 -0.023739101 0.016396184 0.11928964 0.13950661 -0.031624354 -0.01645025 0.14079992 -0.0002824564 -0.08052984 -0.0021310581 -0.025350995 0.086938225 0.14308536 0.17146006 -0.13943303 0.048792403 0.09274929 -0.053167373 0.031103406 0.012354865 0.21057427 0.32618305 0.18015954 -0.15881181 0.15322933 -0.22558987 -0.04200665 0.0084689725 0.038156632 0.15188617 0.13274793 0.113756925 -0.095273495 -0.049490947 -0.10265804 -0.27064866 -0.034567792 -0.018810693 -0.0010360252 0.10340131 0.13883452 0.21131058 -0.01981019 0.1833468 -0.10751636 -0.03128868 0.02518242 0.23232952 0.042052146 0.11731903 -0.15506615 0.0063580726 -0.15429358 0.1511722 0.12745973 0.2576985 -0.25486213 -0.0709463 0.17983761 0.054027 -0.09884228 -0.24595179 -0.093028545 -0.028203879 0.094398156 0.09233813 0.029291354 0.13110267 0.15682974 -0.016919162 0.23927948 -0.1343307 -0.22422817 0.14634751 -0.064993896 0.4703685 -0.027190214 0.06224946 -0.091360025 0.21490277 -0.19562101 -0.10032754 -0.09056772 -0.06203493 -0.18876675 -0.10963594 -0.27734384 0.12616494 -0.02217992 -0.16058226 -0.080475815 0.026953284 0.110732645 0.014894041 0.09416802 0.14299914 -0.1594008 -0.066080004 -0.007995227 -0.11668856 -0.13081996 -0.09237365 0.14741232 0.09180138 0.081735 0.3211204 -0.0036552632 -0.047030564 -0.02311798 0.048961394 0.08669574 -0.06766279 -0.50028914 -0.048515294 0.14144728 -0.032994404 -0.11954345 -0.14929578 -0.2388355 -0.019883996 -0.15917352 -0.052084364 0.2801028 -0.0029121689 -0.054581646 -0.47385484 0.17112483 -0.12066923 -0.042173345 0.1395337 0.26115036 0.012869649 0.009291686 -0.0026459037 -0.075331464 0.017840583 -0.26869613 -0.21820338 -0.17084768 -0.1022808 -0.055290595 0.13513643 0.12362477 -0.10980586 0.13980341 -0.20233242 0.08813751 0.3849736 -0.10653763 -0.06199595 0.028849555 0.03230154 0.023856193 0.069950655 0.19310954 -0.077677034 -0.144811'\n",
    "avg_vec = np.array(vec_string.split(\" \"))\n",
    "unk_df = pd.DataFrame({\"\": np.zeros(300, dtype='float32'),\n",
    "                       \"UNK\": avg_vec}).T\n",
    "pretrained_weights = pd.concat([unk_df, glove_pd])\n",
    "del glove_pd\n",
    "del unk_df\n",
    "pretrained_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = pretrained_weights.loc['UNK', :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {key: val.values for key, val in pretrained_weights.T.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we open the reviews file and turn it into a mappable object. The formatting of the text file has newline characters so we need to split the textfile on '\\n' and ignore the last entry as the file ends with '\\n' and therefore the last element in the list is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with gzip.open('reviews_Home_and_Kitchen_5.json.gz', 'rt') as f:\n",
    "    content = f.read()\n",
    "content = content.split('\\n')[:-1]\n",
    "print(content[0])\n",
    "print(content[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of JSONifiable strings we can turn our reviews into a mapping where each review has a unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews = pd.DataFrame.from_records([json.loads(r) for r in content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the fields, the \"asin\" field could be used to create an average rating per product feature, which could help further down the line to predict a specific rating. Let's do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['avg_product_rating'] = reviews.groupby('asin').overall.transform('mean')\n",
    "reviews = reviews[['asin', 'reviewText', 'overall', 'avg_product_rating']]\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to decide how to preprocess the review texts. First lets check a few stats about the reviews, like max/min length and punctuation count. Not sure yet what that will tell us but could be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "punctuation = string.punctuation\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['review_len'] = reviews.reviewText.progress_apply(len)\n",
    "reviews['punctuation_count'] = reviews.reviewText.progress_apply(lambda x: sum([1 for c in x if c in punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.review_len.hist(bins=100)\n",
    "print(f'Mean review length: {reviews.review_len.mean()}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.punctuation_count.hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the distribution of review lenghts is pareto-like, so most reviews are relatively short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the NLTK word_tokenize and tokenize each review, and check the distribution of number of tokens per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['tokenized_review'] = reviews.reviewText.progress_apply(word_tokenize)\n",
    "reviews['token_count'] = reviews.tokenized_review.progress_apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.token_count.hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean token count: {reviews.token_count.mean()}')\n",
    "print(reviews.loc[reviews.token_count <= 200].token_count.size/reviews.token_count.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution is naturally also pareto-like, which means that we can cap the maximum number of tokens we get embeddings for at a relatively low number without losing too much information. +86% of reviews are no longer than 200 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create embeddings from the tokenized reviews that are padded to a max length of 200 tokens. We'll do this in pure python since I don't want to extract embeddings until after the reviews are padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(some_list, target_len):\n",
    "    return some_list[:target_len] + ['']*(target_len - len(some_list))\n",
    "reviews['tokenized_review_pad'] = reviews.tokenized_review.progress_apply(lambda x: pad_or_truncate(x, MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter()\n",
    "reviews.tokenized_review.progress_apply(lambda x: token_counts.update(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an entry for Out of Vocabulary words using the average vector (from https://stackoverflow.com/questions/49239941/what-is-unk-in-the-pretrained-glove-vector-files-e-g-glove-6b-50d-txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [t for t, v in token_counts.items() if ]\n",
    "vocab2idx = {word: i for i, word in enumerate(vocab)}\n",
    "embeddings = pd.DataFrame(vocab2idx).T.merge(pretrained_weights, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews[['reviewText', 'overall']].to_csv('amazon_for_torchtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', tokenizer_language='en', batch_first=True, include_lengths=True, fix_length=100)\n",
    "LABEL = data.LabelField(dtype=torch.float, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('overall', LABEL), ('reviewText',TEXT)]\n",
    "#loading custom dataset\n",
    "training_data=data.TabularDataset(path='amazon_for_torchtext.csv', format='csv', fields=fields, skip_header=True, )\n",
    "\n",
    "#print preprocessed text\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print preprocessed text\n",
    "print(vars(training_data.examples[5000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = training_data.split(split_ratio=0.7, random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize glove embeddings\n",
    "TEXT.build_vocab(train_data, min_freq=3, vectors=\"glove.840B.300d\")  \n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "#No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "\n",
    "#No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
    "\n",
    "#Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))  \n",
    "\n",
    "#Word dictionary\n",
    "print(TEXT.vocab.stoi)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "#set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.reviewText),\n",
    "    sort_within_batch=True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    \n",
    "    #define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        #Constructor\n",
    "        super().__init__()          \n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        #dense layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        #activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent_len, emb dim]\n",
    "      \n",
    "        #packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\n",
    "        \n",
    "        #concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.act(dense_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hyperparameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 6\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.2\n",
    "\n",
    "#instantiate the model\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \n",
    "                   bidirectional = True, dropout = dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#architecture\n",
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#define metric\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "    \n",
    "#push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    #initialize every epoch \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #set the model in training phase\n",
    "    model.train()  \n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        #resets the gradients after every batch\n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        #retrieve text and no. of words\n",
    "        text, text_lengths = batch.reviewText   \n",
    "        \n",
    "        #convert to 1D tensor\n",
    "        predictions = model(text, text_lengths).squeeze()  \n",
    "        \n",
    "        #compute the loss\n",
    "        loss = criterion(predictions, batch.overall)        \n",
    "        \n",
    "        #compute the binary accuracy\n",
    "        acc = binary_accuracy(predictions, batch.overall)   \n",
    "        \n",
    "        #backpropage the loss and compute the gradients\n",
    "        loss.backward()       \n",
    "        \n",
    "        #update the weights\n",
    "        optimizer.step()      \n",
    "        \n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()  \n",
    "        epoch_acc += acc.item()    \n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    #initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    #deactivating dropout layers\n",
    "    model.eval()\n",
    "    \n",
    "    #deactivates autograd\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "        \n",
    "            #retrieve text and no. of words\n",
    "            text, text_lengths = batch.reviewText\n",
    "            \n",
    "            #convert to 1d tensor\n",
    "            predictions = model(text, text_lengths).squeeze()\n",
    "            \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.overall)\n",
    "            acc = binary_accuracy(predictions, batch.overall)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "     \n",
    "    #train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    #evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW 5 NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
